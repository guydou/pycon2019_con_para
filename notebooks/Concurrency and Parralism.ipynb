{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concurrency and Parralism in Python\n",
    "\n",
    "## pyconil 2019\n",
    "### by Guy Doulberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Talk Metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Who am I?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* My name is Guy Doulberg\n",
    "* 15 years of development experience\n",
    "* I have been developing in python in the last 3 years\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Working for [Satellogic](https://satellogic.com/) \n",
    "* A startup that want to capture frequent high quality images of the earth\n",
    "* We use satellites to produce images\n",
    "* I am resposible for some of the data processing that is being done after the cpature was taken\n",
    "* We used the following methods in our system.\n",
    "\n",
    "An image that was captured by one of Satellogic's sattellites \n",
    "![satellogic](https://satellogic.com/wp-content/themes/satellogic-theme/dist/assets/images/what-we-do.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why am I doing this talk?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Because it is important for every python developer\n",
    "* Becasue I was happy to hear this talk about 2 years ago when I had the need to concurrency in python\n",
    "* Because it is fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What am I going to talk about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What is Concucurency and Parralism\n",
    "* How to implement softwares that use Concurrency and Parraslism in the python stadard library \n",
    "* How to implement Concurrency and Parraslism acrross hosts (cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The examples you are going to see are made up, synthesised to make points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Which python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* python 3.7.3\n",
    "* I don't know enough about python 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resources:\n",
    "\n",
    "* Git repo: https://github.com/guydou/pycon2019_con_para\n",
    "* Slides: https://guydou.github.io/pycon2019_con_para/index.html#/ ![QRCODE](https://guydou.github.io/pycon2019_con_para//images/qrcode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theortical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Concurrency\n",
    "\n",
    "Consider you have two or more tasks that you need to execute on the same time. The tasks can start and be executed on the same time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Parralism \n",
    "\n",
    "Consider you have two or more tasks that run exactly on the same time (in parralel paths)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](http://alvinalexander.com/sites/default/files/photos/parallelism-vs-concurrency.png)\n",
    "\n",
    "image taken from \"Parallelism vs concurrency in programming\" [link](https://alvinalexander.com/photos/parallelism-vs-concurrency-programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# IO bound vs CPU bound\n",
    "\n",
    "When considering optimiziation of a code block by either running the code in parrallel or concurrently, you should indentify your **bottleneck** properly. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If your program can run faster when using more computing units (CPUs) - your program is considered to be **CPU bound**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If your program can run faster when using more bandwidth or reading/writing to/from several sources. Your program is considered to be **IO bound**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is important to identify the nature of your program becasue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Adding more computing units to IO bounded program will not help -  (might do worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Running a CPU bounded program using a single CPU is a sub-optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Memory Bounded \n",
    "\n",
    "\n",
    "* A program can also be bounded by the available memory of the machine\n",
    "\n",
    "* If you program can run faster when using more memory (RAM)\n",
    "\n",
    "# Stay Tuned:\n",
    "\n",
    "* In one of the last slides I will show how to use cluster of machines, that could be a solution for such programs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with IO Bounded tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Basically when dealing with IO bounded tasks I would like to break the code that is boudned by IO and run it *Concurrently*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Physics \n",
    " * In a single machine you can't utilize more than your avialable disk I/O\n",
    " \n",
    " * In a single machine you can't utilize more than your avialable network I/O\n",
    " \n",
    " * In a client server architecture, a client can't work faster than the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's do it in python\n",
    "\n",
    "First I will create a usecase of I/O bounded task,\n",
    "\n",
    "A flask server that runs the following code:\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "import time\n",
    "\n",
    "@app.route(\"/\")\n",
    "def sleep_well():\n",
    "    sleep_duration = 2\n",
    "    time.sleep(sleep_duration)\n",
    "    return \"Hello World!\"\n",
    "```\n",
    "\n",
    "I am running this flask server behind a gunicorn, in order to control the number of requests it can handle:\n",
    "\n",
    "```shell\n",
    "gunicorn --bind 0.0.0.0:5000  -w 10 wsgi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Naive implemetation will be **sequential** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 20.102389812469482\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "host = \"http://localhost:5000\"\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "    body = requests.get(host)\n",
    "    assert body.text == \"Hello World!\"\n",
    "    \n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using concurrent.futures.ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **ThreadPoolExecutor** - An Executor subclass that uses a pool of at most max_workers threads to execute calls asynchronously ....\n",
    "* **as_completed** - Returns an iterator over the Future instances (possibly created by different Executor instances) given by fs that yields futures as they complete (finished or were cancelled). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The code I want to run concurrently is the code that is actaully doing the calls to the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def io_bounded_task(host):\n",
    "    body = requests.get(host)\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now I am going to submit this code 10 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## The thread pool will exectute the code concurrently "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## And my code will wait for all the executions to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 2.0783989429473877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as e:\n",
    "    futures = {e.submit(io_bounded_task, host) for i in range(10)}\n",
    "    for future in as_completed(futures):\n",
    "        assert future.result().text == \"Hello World!\"\n",
    "    \n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What happens when we reach the limit?\n",
    "\n",
    "* Depends on what is blocking us\n",
    "* If we just exceedrd the available bandwidth (by the server/client ISP, network card) - we just wouldn't be able to run more concurrent tasks\n",
    "* We could also be blocked by the server we are trying to reach \n",
    "\n",
    "It might make sense to **throttle** our requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In the following code we run 20 threads while our server can handle only 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 4.043481826782227\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with ThreadPoolExecutor() as e:\n",
    "    futures = {e.submit(io_bounded_task, host) for i in range(20)}\n",
    "    for future in as_completed(futures):\n",
    "        assert future.result().text == \"Hello World!\"\n",
    "    \n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "# we are being throttled by the server so it will not be broken\n",
    "#It is not healthy to count on the server for these things, we should count on our own\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It doesn't make sense to run with 20 threads lets run only with 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 4.078224420547485\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=10) as e:\n",
    "    futures = {e.submit(io_bounded_task, host) for i in range(20)}\n",
    "    for future in as_completed(futures):\n",
    "        future.result().text == \"Hello World!\"\n",
    "    \n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# asyncio\n",
    "## When dealing with IO bounded tasks we can also use the asyncio module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## asyncio theory \n",
    "\n",
    "1. The idea is that there is an event loop(main thread), \n",
    "\n",
    "1. The event loop deligates iobounded tasks to other threads and continues with its exectution sequence. \n",
    "\n",
    "1. When result is ready, the event loop goes back to the the code that called the task and continues its sequence.\n",
    "\n",
    "1. The event loop decides when and which code it runs, the code it runs must not block! \n",
    "\n",
    "1. Conisder we have 10 non blocking methods that fetch data from a server, the main loop runs them concurrently. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## define a coroutine that accesses the host\n",
    "A function that can be entered and exited multiple times, suspended and resumed each time, is called a coroutine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "async def async_io_bounded_task(host):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(host) as response:\n",
    "            text = await response.read()\n",
    "            assert text == b\"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## await for all the corutines to return\n",
    "\n",
    "* Await keyword works only if you are in the an active event loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# creating a coroutine mehtod to run several corutines\n",
    "async def main():\n",
    "    tasks = []\n",
    "    await asyncio.gather(*[async_io_bounded_task(host) for i in range(10)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create an event loop\n",
    "\n",
    "The event loop is the core of every asyncio application. Event loops run asynchronous tasks and callbacks, perform network IO operations, and run subprocesses.\n",
    "\n",
    "Application developers should typically use the high-level asyncio functions, such as asyncio.run(), and should rarely need to reference the loop object or call its methods. This section is intended mostly for authors of lower-level code, libraries, and frameworks, who need finer control over the event loop behavior.\n",
    "\n",
    "In this example I am using the event loop of jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 2.018639087677002\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# I am running in a jupyter notebook so I have already event loop running:\n",
    "await main()\n",
    "\n",
    "# if you don't have event loop then:\n",
    "# asyncio.run(main())\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Which module to choose?\n",
    "\n",
    "I think it is a matter of taste, \n",
    "\n",
    "Maybe I am old fashined, but I prefer the the Exector module becasue it is easier for me to folow the code path \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPU bounded jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## For the propuse of the talk I am going to produce a big array of random floats\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "random_numbers = numpy.random.random(300000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I am using numpy to speed things up, \n",
    "\n",
    "* The numpy object is also an object which is easier to share accross processes - I will use this feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's  try to get the maximal value out of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# First Attempt: sequential approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values is 0.9999999963405415 \n",
      "Total time of execution: 15.650021076202393\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"Max values is %s \" % max(random_numbers))\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "print(\"Total time of execution: %s\" % (total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CPU utilization when running the sequential max:\n",
    "\n",
    "![image](https://guydou.github.io/pycon2019_con_para//images/max_seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lets use threads\n",
    "\n",
    "\n",
    "## From the pthread library of C/C++ description:\n",
    "\n",
    "```\n",
    "... It is most effective on multi-processor or multi-core systems where the process flow can be scheduled to run on another processor thus gaining speed through parallel or distributed processing...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Proposed implementation:\n",
    "\n",
    "1. Split the list to chunks\n",
    "\n",
    "1. Find the max value in each chunk\n",
    "\n",
    "1. Find the maximal value in all maximal values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First lets check how much time it takes to run on a single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of execution: 0.16901397705078125\n"
     ]
    }
   ],
   "source": [
    "num_chunks = 100\n",
    "chunk_number = 10\n",
    "def slice_chunk(chunk_num, array, num_chunks):\n",
    "    offset = int(len(array)/num_chunks)\n",
    "    return array[chunk_number*offset:(chunk_number+1)*offset]\n",
    "\n",
    "start_time = time.time()\n",
    "max(slice_chunk(chunk_number, random_numbers, num_chunks))\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Can running on these kind of chunks in parllel can help us?**\n",
    "\n",
    "1. We have 4 cpus\n",
    "1. Finding a max value out of a chunk take 0.2 seconds\n",
    "1. We have 100 chunks\n",
    "1. We can run 25 groups of chunk in parallel \n",
    "\n",
    "The total time of execution should be:\n",
    "```python\n",
    "25*0.2 = 5 #seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run the on chunk in parllel wait for thre max values to return and find the maximal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values is 0.99999934045655 \n",
      "Total time of execution: 16.2425696849823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor() as e:\n",
    "    futures = {e.submit(max, slice_chunk(chunk_number, random_numbers, num_chunks)) for chunk_number in range(num_chunks)}\n",
    "    max_values = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "print(\"Max values is %s \" % max(max_values))\n",
    "        \n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![oops](https://thumbs.dreamstime.com/z/vector-oops-symbol-over-white-29840798.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Say hello to the GIL:  or Global Interperter lock\n",
    "\n",
    "* The GIL is a mutex (lock) that prevent access to python objects from different threads.\n",
    "\n",
    "* In our case we need access from different threads, but the GIL in practice makes the threds code to run sequetialy using a single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CPU utilization when running mutlithreading code:\n",
    "\n",
    "![image](https://guydou.github.io/pycon2019_con_para/images/threads_max.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lets try the same thing using processes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values is 0.99999934045655 \n",
      "Total time of execution: 8.223525524139404\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()    \n",
    "\n",
    "with ProcessPoolExecutor() as e:\n",
    "    futures = {e.submit(max, slice_chunk(chunk_number, random_numbers, num_chunks)) for chunk_number in range(num_chunks)}\n",
    "    max_values = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "print(\"Max values is %s \" % max(max_values))\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "              \n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why is it not faster\n",
    "\n",
    "* Spawning proceses is an heavy task \n",
    "* Marshaling/Unmarshaling of chunk of data is heavy\n",
    "\n",
    "## What to do?\n",
    "\n",
    "* Initialize the processes in advance and reuse the proceses\n",
    "* Use shared memory and pointer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import SharedArray as sa\n",
    "array_key = \"shm://test\"\n",
    "sa.delete(array_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import SharedArray as sa\n",
    "array_key = \"shm://test\"\n",
    "\n",
    "array = sa.create(array_key, random_numbers.shape, random_numbers.dtype)\n",
    "array[:] = random_numbers[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now we need to retrieve the chunk of array from the from the shared array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def max_shared_array(chunk_num, array_key, num_chunks):\n",
    "    array = sa.attach(array_key)\n",
    "    return max(slice_chunk(chunk_num, array, num_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values is 0.99999934045655 \n",
      "Total time of execution: 6.6549787521362305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with ProcessPoolExecutor() as e:\n",
    "    futures = {e.submit(max_shared_array, chunk_number, array_key, num_chunks)\n",
    "               for chunk_number in range(num_chunks)}\n",
    "    max_values = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "print(\"Max values is %s \" % max(max_values))\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "\n",
    "\n",
    "              \n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CPU utilization when running multiprocess code:\n",
    "\n",
    "\n",
    "![image](https://guydou.github.io/pycon2019_con_para/images/processes_max.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now lets check what happens when running with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max values is 0.9999999998221734 \n",
      "Total time of execution: 0.1670057773590088\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "    \n",
    "\n",
    "print(\"Max values is %s \" % numpy.max(random_numbers))\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "              \n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CPU Bound conclusion:\n",
    "\n",
    "1. Don't forget the GIL\n",
    "2. Multiprocessing is your friend\n",
    "3. Use native C/C++ labraries suche as: numpy, pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask\n",
    "\n",
    "\n",
    "* **Dynamic task scheduling optimized for computation**\n",
    "* **“Big Data” collections**\n",
    "\n",
    "\n",
    "![architecture](https://docs.dask.org/en/latest/_images/collections-schedulers.png)\n",
    "\n",
    "* In other words, you can run a complex excution grpah on many data types using the same code on you laptop and on a multi host cluster\n",
    "\n",
    "* A way to solve memory bounded problems\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### I will demo only the library **dask.distirbuted** that can help you run you code utilizing CPUs and Hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cluster setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37657\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.68 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:37657' processes=4 cores=4>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # set up local cluster on your laptop\n",
    "# client = Client(Cluter Host) \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lets try to find the maximal value out a the randon_numbers. \n",
    "\n",
    "\n",
    "1. Find the maximal value in each chunk\n",
    "\n",
    "1. Find the maximal value out of all maximal values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def dask_mask_array(chunk_num):\n",
    "    array_key = \"shm://test\"\n",
    "    return max_shared_array(chunk_num, array_key, num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999999955677862\n",
      "Total time of execution: 7.13366961479187\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "max_values = client.map(dask_mask_array, range(num_chunks) )\n",
    "max_value = client.submit(max, max_values)\n",
    "\n",
    "print(max_value.result())\n",
    "\n",
    "end_time = time.time()\n",
    "total = end_time - start_time\n",
    "print(\"Total time of execution: %s\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CPU utilization when running dask code:\n",
    "\n",
    "\n",
    "![image](https://guydou.github.io/pycon2019_con_para/images/dask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
